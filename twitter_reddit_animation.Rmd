---
title: "Leafs Chatter Charts"
output: html_document
---

# Animation Notebook

In this notebook, I use R & Python to scrape Leafs comments off Twitter and Reddit, scrape relevant game metadata from various sources (ex. goal timestamps, team colours, start of game timestamps) by cleaning website data using regular expressions, then I combine the comments with the metadata and create an animated graph that shows what both audiences are talking about every two mins as they watch the hockey game unfold.

# Packages

You'll need to have python installed. I [followed this article](https://docs.python-guide.org/starting/install3/win/) on how to install it on Windows 10 using `choco`.

```{r packages, message=FALSE, warning=FALSE}
library(tidyverse) # for general data manipulation in R
library(reticulate) # to convert from Python
library(rtweet) # to get tweets
library(lubridate) # for date manipulation
library(tidytext) # for tokenizing text & tf-idf
library(gganimate) # for chart animation
library(rvest) # for scraping website html
library(png) # for image manipulation
library(grid) # for custom plot manipulation
library(extrafont) # for nice fonts
library(ggtext) # for adding html styling in plot titles
theme_set(theme_light(base_family = "Montserrat ExtraBold"))
# py_install("pandas") # run this to install pandas
# py_install("praw") # and praw
```

# User Inputs

I got a few questions to ask...

```{r input}
# When was the game played?
game_date <- "2020-03-10"

# What keywords should the Leafs tweets contain?
tweet_keywords <- "#TMLTalk OR #LeafsForever OR @MapleLeafs OR leafs OR #leafs OR #GoLeafsGo OR #mapleleafs"

# What is the url to the reddit game thread?
reddit_url <- "https://www.reddit.com/r/leafs/comments/fgl1tq/game_thread_tampa_bay_lightning_43206_at_toronto/"
```

# Gather metadata about the game

## Fetch original schedule data

First, I scraped the Leafs upcoming schedule back in November 2019 off [CBS sports](https://www.cbssports.com/nhl/teams/TOR/toronto-maple-leafs/schedule/regular/) and have that stored. The pre-game schedule has the our `game_start` time which I will use to filter out the comment/tweets data and define the outer limits of the chart.

```{r}
# get the base schedule stored in the repo that has the pre-game data saved to join
pre_game_schedule <- read_csv(str_glue("{here::here()}/team_schedules/leafs_game_schedule.csv")) %>%
  select(date, game_start, opponent = opp) # our columns
```


But the website updates when a game is played to show the `game_result`. So once a game is played, the website replaces `game_start` time for the `game_result` that's why we will scrape the page today and join it with our saved version. 

## Fetch the latest data from the site

```{r schedule_data, message = FALSE}
page_url <- "https://www.cbssports.com/nhl/teams/TOR/toronto-maple-leafs/schedule/regular/"

# check if it's legal to scrape
robotstxt::paths_allowed(page_url)

# scrape tables off the page
cbs_leafs_tables <- read_html(page_url) %>%
  html_table()

# get the post-game data
completed_game_schedule <- cbs_leafs_tables[[1]] %>% 
  janitor::clean_names() %>% # make the column names snake_case
  mutate(date = mdy(date)) %>%
  select(date, result) # get the score (result) to add to the pre-game schedule
```

When I join the post-game data back into the pre-game data I stored, I can leverage that information throughout this workbook.

```{r schedule_data, message = FALSE}
# join results table into the original leafs schedule
game_results_df <- pre_game_schedule %>%
  left_join(completed_game_schedule, by = "date") %>%
  mutate(win = ifelse(str_detect(result, "^W"), TRUE, FALSE))

# filter for the game of interest using date == game_date (an variable we specified earlier)
game_data <- game_results_df %>%
  filter(date == game_date)
```

# Get our Data from Reddit

## Establish connection to Reddit

You'll also need to visit the developer reddit page and create an app to get details for the token below.

[https://www.reddit.com/prefs/apps](https://www.reddit.com/prefs/apps)

```{r}
# get my tokens and secrets stored in my .Renviron
user_agent_reddit <- Sys.getenv("USER_AGENT_REDDIT")
client_id_reddit <- Sys.getenv("CLIENT_ID_REDDIT")
client_secret_reddit <- Sys.getenv("CLIENT_SECRET_REDDIT")
```

Using `{reticulate}`, regular R objects can be fetched in Python chunks using `r.` so when I call `r.reddit_url` it has the string we defined in the R chunk `# Inputs`.

```{python reddit_connection}
import pandas as pd # to create pandas data
import praw # to connect to Reddit app

# replace with your own info
reddit = praw.Reddit(client_id=r.client_id_reddit, 
  client_secret=r.client_secret_reddit, 
  user_agent=r.user_agent_reddit)
```

## Scrape reddit game thread

This will take a few minutes.

```{python reddit_scrape}
# grab the thread that you put in the url argument in the Inputs section
submission = reddit.submission(url=r.reddit_url)

# create an empty dictionary to insert our for-loop results in
topics_dict = {"body":[], "created":[]}

# request the body (text) and created (time of post) elements from our submission (the url we provided above)
submission.comments.replace_more(limit=None)
for comment in submission.comments.list():
  topics_dict["body"].append(comment.body)
  topics_dict["created"].append(comment.created)

# convert the dictionary into a data frame
topics_data = pd.DataFrame(topics_dict)
```

Inversely, I can use `{reticulate}` to call `py$` and examine a python object in an R chunk. When I call `py$topics_data` it is converted from a pandas dataframe into an R dataframe.

```{r reddit_data_complete}
# bring the topics_data panada dataframe into R and clean
tml_full_reddit_set <- py$topics_data %>%
  mutate(created = ymd_hms(as_datetime(created) - hours(12), tz = "America/New_York")) %>% # adjust the timezone to EST
  rename(text = body,
         created_at = created) # make the date & text variable consistent on reddit with twitter

# make friendly formats to save my files
opponent_file_format <- str_replace_all(tolower(game_data$opponent), " ", "-")

# save the comments under the opponent's name and date of the game
write_csv(tml_full_reddit_set, str_glue("reddit_posts/{opponent_file_format}-{game_date}.csv"))
```

# Get our data from Twitter

You'll need to apply for a Twitter API Token. More details at: [https://developer.twitter.com/](https://developer.twitter.com/).

Please consult [rtweet's vignette](https://rtweet.info/index.html) for more info on setting up the token.

```{r}
# substitute your own information below
twitter_token <- create_token(app = Sys.getenv("TWITTER_APP"), 
                              consumer_key = Sys.getenv("TWITTER_CONSUMER_KEY"), 
                              consumer_secret = Sys.getenv("TWITTER_CONSUMER_SECRET"))
```

## Get Tweets that use our keywords

Scrape all tweets from that day that include the keywords we specified above. These do not include retweets so popular tweets do not heavily influence the TF-IDF formula.

```{r leafs_tweets, message = FALSE}
# if you've already scraped the data, you won't need to do it again if you run all chunks
if(file.exists(str_glue("{here::here()}/tweets/tml_talk-{opponent_file_format}-toronto-{game_date}.csv")) == FALSE) {

since <- as.Date(game_date) #get the data since today and
until <- since + days(2) # until two days after so i get all of yesterday

# save tweets locally
search_tweets(tweet_keywords,
              since = since,
              until = until,
              type = "recent",
              include_rts = FALSE,
              retryonratelimit = TRUE,
              token = twitter_token
              ) %>%
  mutate_if(is.list, as.character) %>%
  write_csv(
    here::here(str_glue("tweets/tml_talk-{opponent_file_format}-toronto-{game_date}.csv")) # save
  )

}

# read the saved data
tml_talk <- read_csv(here::here(str_glue("tweets/tml_talk-{opponent_file_format}-toronto-{game_date}.csv"))) %>%
    select(created_at, text)
```

## Add influencers

Sometimes general keywords miss valuable conversations. 

I chose some people who commonly tweet about the Leafs during games to add into the data.

```{r influencer_tweets, message=FALSE}
tmls <- get_timelines(c("domluszczyszyn", "Steve_Dangle",
                        "IanGraph", "JeffVeillette",
                        "DownGoesBrown", "3rdPeriodSuits",
                        "ThatsCappy", "duarteelauraa",
                        "rahef_issa", "_marlanderthews", 
                        "LeafFan1917", "TheLeafsIMO", 
                        "TheOakLeafs", "TheFlintor", 
                        "MarkUkLeaf", "LeafsMaz20",
                        "karlandtheleafs", "Buds_All_Day",
                        "mirtle", "jonassiegel",
                        "kristen_shilton", "reporterchris",
                        "51Leafs", "ATFulemin",
                        "draglikepull", "TLNdc",
                        "TicTacTOmar", "PPPLeafs",
                        "MatthewsIsALeaf", "LeafsAllDayy",
                        "NickDeSouza_", 
                        "thejustinfisher", "HardevLad",
                        "RyanDHobart", "jpolly22",
                        "jxcquelineoh", "jakebeleafs",
                        "account4hockey", "briancrd", 
                        "team_keefe", "Dylan_Morrow",
                        "LeafsFansUnited", "aigemac",
                        "mostlyleafies", "Its_Mr_Clutch"), 
                      n = 100,
                      token = twitter_token) %>% # 100 tweets each
  select(created_at, text)
```

## Combine the tweet

```{r all_tweets}
# join the timelines of popular leaf accounts into the "leafs" tweet corpus
tml_full_tweet_set <- tml_talk %>%
  full_join(tmls) %>%
  group_by(created_at, text) %>%
  filter(row_number() == 1) %>% # remove duplicate entries found in both datasets
  ungroup() %>%
  mutate(created_at = with_tz(created_at, tzone = "America/New_York")) # set to EST
```

# Processing the data from both sources

## Step 1: Cut data into two minute intervals

Reduce the data into the time frame we are interested in. 

### Define time frame

```{r}
# get puck drop time
puck_drop <-  ymd_hms(game_data$game_start, tz = "America/New_York")

min_hour <- puck_drop - 900 # filter 15 mins before scheduled game start
max_hour <- puck_drop + 10800 
```

### Cut into intervals


```{r}
# define a function that filters down timestamps only to what we desire and rounds them
reduce_to_intervals <- function(full_comment_df) {
  full_comment_df %>%
    filter(created_at > min_hour,
           created_at < max_hour) %>%
    mutate(interval = round_date(created_at, "2 mins")) # round into 2 minute intervals
}
```

```{r}
tml_reddit_intervals <- reduce_to_intervals(tml_full_reddit_set)

tml_tweet_intervals <- reduce_to_intervals(tml_full_tweet_set)
```

## Step 2: Define unwanted words

These are specifically unspecific words and keywords I use in my twitter query. I add more to these lists if they are boring words.

```{r}
# Some unwanted_words and the search terms used
twitter_unwanted_words <- c("10u", "t.co", "gotta", "#leafsforever", "games", "leafs", "#tmltalk", 
                            "hockey", "dont", "amp", "period", "region", "https", "10a", "pas", "att", 
                            "gonna", "ive", "les", "game", "hes", "#leafs", "#goleafsgo", "leaf's", "vai", 
                            "ml\U0001f4b5", "lieut", "maple", "vous", "weve", "ill", "theyre", "isnt", 
                            "youre", "o55", "bla", "guys", "row", "usa", "och", "temporada", "07mar",  
                            "playing", "play", "plays", "taking", "happen", "people", "leafs", "teams", 
                            "team", "people", "game", "gonna", "looked", "stream", "literally", "arent", 
                            "played", "10mar", "bay", "cuz", "sur", "didnt", "doesnt", "watch")

reddit_unwanted_words <- c("leafs", "teams", "team", "people", "game", "playing", "play", "plays", "taking", 
                           "happen", "people", "gonna", "looked", "stream", "literally", "arent", "played", "guys", 
                           "period", "dun", "they’re", "makes", "periods", "players", "watch", "lot", "san", "based")
```

# Functions for processing tweets & reddit comments

We do the same processing to both data sets. So we should make some functions. Details are commented in the code.

```{r}
# tokenize the data into intervals
create_two_min_tokens <- function(interval_df, tokenization = "words", unwanted_words) {
  interval_df %>%
    unnest_tokens(word, text, token = tokenization) %>%
    mutate(word = str_replace_all(word, "[,_;\\.?!]", " "),
           word = str_replace_all(word, ":$", " "),
           word = str_replace_all(word, "\\\n", " "),
           word = str_remove_all(word, '[”"“]'),
           word = str_remove_all(word, "’$"),
           word = str_remove_all(word, "’s$"),
           word = str_remove_all(word, "’t$"), # lots of random cleaning from bad apostrophes
           word = str_remove_all(word, "’m$"),
           word = str_remove_all(word, "'$"),
           word = str_trim(word)) %>%
    filter(!word %in% unwanted_words, # drop unwanted words 
           nchar(word) > 2, # words must be longer than 2 characters
           !str_detect(word, "^@"), # no twitter handles
           !str_detect(word, "[0-9]+")) %>% # no long numbers
    anti_join(stop_words, by = "word")  # removes noisy words
}
```


```{r}
# supply the interval dataframe, the unwanted word vector, and tokenize tweets differently
reddit_two_min_tokens <- create_two_min_tokens(interval_df = tml_reddit_intervals, 
                                               unwanted_words = reddit_unwanted_words) %>%
  count(word, interval) 

twitter_two_min_tokens <- create_two_min_tokens(interval_df = tml_tweet_intervals, 
                                                unwanted_words = twitter_unwanted_words,
                                                tokenization = "tweets")  %>%
  count(word, interval)
```

We will count the words in the tweets and apply tf-idf every two minute interval.

```{r}
find_important_words <- function(tokenized_df, threshold = 3){
  tokenized_df %>%
    bind_tf_idf(word, interval, n) %>%
    filter(n >= threshold) %>% # word must appear 3 times in the interval to qualify
    arrange(interval, desc(tf_idf)) %>% # arrange by tf_idf
    distinct(interval, .keep_all = T) %>% # get the top result
    arrange(interval)
}

tml_top_words_reddit <- find_important_words(reddit_two_min_tokens, threshold = 2)

tml_top_words_tweets <- find_important_words(twitter_two_min_tokens, threshold = 3)
```

```{r}
calculate_interval_volume <- function(interval_df) {
  interval_df %>% 
    group_by(interval) %>%
    summarize(thread_volume = n()) %>%
    ungroup()
}

# use the interval dataframe again to count the volume in each interval
tml_reddit_volume <- calculate_interval_volume(tml_reddit_intervals)

tml_tweet_volume <- calculate_interval_volume(tml_tweet_intervals)
```

# Add volume to intervals

## Reddit

We need to `full_join` because tf-idf can fail to find a word and we want to keep those instances so we can forward fill with the previous word unless the thread_volume drops below 5 posts a second.

```{r}
full_reddit_data <- tml_reddit_volume  %>%
  full_join(tml_top_words_reddit, by = "interval") %>% 
  fill(word) %>% 
  mutate(word = replace_na(word, ""),
         word = ifelse(thread_volume < 5, "", word))
```

## Twitter

```{r}
full_tweet_data <- tml_tweet_volume  %>%
  inner_join(tml_top_words_tweets, by = "interval") 
```

# Extract goals from @MapleLeafs' timeline 

Ok, now we have our two sources of comments, let's focus on getting our goal events and game start/end timestamps for the chart.

## Scrape the Leafs timeline

First off, the Leafs' social media manager tweets in a similar fashion game-to-game.

That means We can reliably regex the approximate game start, game end, and goal announcements in real time instead of using game time.

```{r getting_leafs_timeline, message = FALSE}
# get the recent 100 tweets by the leafs official account
leafs_timeline <- get_timeline("@MapleLeafs", n = 2500, token = twitter_token) %>%
  select(status_id, created_at, text) %>%
  mutate(created_at = with_tz(ymd_hms(created_at), tzone = "America/New_York")) %>%
  filter(created_at >= game_data$date)
```

## Find Goals

MapleLeafs always tweet "GOAL" when they score.

Sometimes they may not though, so i've added an argument `missing_goal_tweet_ids` for someone to put a vector of any that don't get picked up.

```{r leaf_goals}
# time of leafs goals
find_leafs_goals <- function(tweet_timeline, missing_goal_tweet_ids = c()){
  tweet_timeline %>%
    filter(str_detect(text, "GOAL") | status_id %in% missing_goal_tweet_ids) # detect tweets that contain the word GOAL
}

leafs_goals <- find_leafs_goals(leafs_timeline)
```

The tweets very often mention the city/team name of an opponent when they score. But not always. If we are getting blown out they'll just tweet the score and they commonly just say "Empty net goal" if one is scored.

```{r opponent_goals}
# regex the opponent's city name (ex. Calgary scores) and team name (ex. Flames score) to help search for their goal announcements
find_opponent_goals <- function(tweet_timeline, missing_goal_tweet_ids = c()){
  
  opponent_city <- str_extract(str_glue("{game_data$opponent}"), "^[a-zA-Z]+") # get first word of opponent
  opponent_name <- str_extract(str_glue("{game_data$opponent}"), "[a-zA-Z]+$") # get last word iof opponent
  
  tweet_timeline %>%
    filter(
      ((str_detect(text, opponent_city) | str_detect(text, opponent_name) | str_detect(text, "Empty net goal")) # tweet contains one of these phrases
       & str_detect(text, "score") | str_detect(text, "Empty net goal")) # and either of these phrases
      | (status_id %in% missing_goal_tweet_ids))
}

opponent_goals <- find_opponent_goals(leafs_timeline)
```

You'll need to manually add the goals the regex misses. This can happen with teams like LA or Habs. I recommend you explore the leafs_timeline object and remove the filter statement above and hardcode something like `filter(status_id == "1236126442288709633")`.

## Game start 

The Leafs' social media manager also usually announces the game start in their tweet with a common set of phrases. But if it is wrong, we can specify the id manually.

```{r game_start_and_end}
# Find the game start tweet using these keywords, if needed use manual_id to specify
find_real_game_start <- function(tweet_timeline, manual_id = NULL) {
  if (!is.null(manual_id)) {
  tweet_timeline %>%
    filter(status_id == manual_id)
  } else {
  tweet_timeline %>%
    filter(str_detect(text, "[Aa]ction") | str_detect(text, "[Uu]nder way") | str_detect(text, "[Tt]une in") | str_detect(text, "(go time)")) %>%
    arrange(created_at) %>%
    head(1)
  }
}

game_start_tweet <- find_real_game_start(leafs_timeline)
```

## Game end

If we win, they always say "LEAFS WIN". Otherwise, there's a mixute of other phrases when we lose. 

We will build some regex to find these most times. Worst case we need to search for the correct posts and hardcode.

```{r game_start_and_end}
find_real_game_end <- function(tweet_timeline, manual_id = NULL) {
  if (!is.null(manual_id)) {
    tweet_timeline %>%
      filter(status_id == manual_id)
  } else {
    if (game_data$win == TRUE) {
      tweet_timeline %>%
        filter(str_detect(text, "LEAFS WIN"))
    } else {
      tweet_timeline %>%
        filter(str_detect(text, "[Tt]ough") | str_detect(text, "[Bb]attle") | str_detect(text, "[Ff]inal")) %>%
        head(1)
    }
  }
}

end_of_game <- find_real_game_end(leafs_timeline)
```

# Make the chart

## Fetch the background logos

I add transparent logos to the background image I scraped all them from Wikipedia profile images using the code in the setup_notebook repo.

```{r leaf_logo}
# leafs colour
leafs_blue <- "#00205B"

# read the leafs logo
l <- readPNG(str_glue("{here::here()}/team_images/220px-Toronto_Maple_leafs_2016_logo.svg.png"))

# I make them transparent by multiplying the RGB values by 0.2.
f <- matrix(rgb(l[,,1],l[,,2],l[,,3], l[,,4] * 0.4), nrow=dim(l)[1]) # This makes it transparent - 0.2 is alpha
leafs_logo <- rasterGrob(f, interpolate=TRUE)
```

```{r opponent_logo}
# lookup all team image files in the repo
files <- file.info(list.files(str_glue("{here::here()}/team_images"), 
                              full.names = TRUE))

image_paths <- data.frame(path = rownames(files))

# look for the opponent's image 
opponent_image <- image_paths %>%
  mutate_if(is.factor, as.character) %>%
  filter(str_detect(path, str_replace_all(game_data$opponent, " ", "_")))

# opponent logo
m <- readPNG(opponent_image$path)

# make transparent
w <- matrix(rgb(m[,,1],m[,,2],m[,,3], m[,,4] * 0.4), nrow=dim(m)[1]) 
opponent_logo <- rasterGrob(w, interpolate=TRUE)
```

## Lookup opponent's colour

The Leafs are always blue. I use [teamcolorcodes.com](teamcolorcodes.com) to lookup the opponent's colour. This is for the goal number to better stylize the chart.

```{r}
# build url
team_url <- paste0("https://teamcolorcodes.com/",  tolower(opponent_file_format), "-color-codes/")

# check if legal to scrape
robotstxt::paths_allowed(team_url)

# get text from the website
page_text <- read_html(team_url) %>%
  html_nodes("body") %>%
  html_nodes("div") %>%
  html_text() 

opponent_colour <- str_extract(page_text[9], "#[a-zA-Z0-9]+")
```

## Make clean x-axis breaks

Make the x-axis breaks look human readable.

```{r}
# x-axis time stamps
hourly_ranges <- data.frame(time = seq(min_hour + 900, max_hour + 1800, by = 3600)) # every hour

# convert into human readable labels
time_breaks <- hourly_ranges %>%
  mutate(time = format(strptime(hourly_ranges$time, "%F %H:%M:%S"), format = "%I:%M %p"), # delete this $
         time = str_remove(time, "^0"),
         time = str_remove(time, " "),
         time = toupper(time))
```

## Scoreboard data

Make a dataframe with the current game score to plot.

```{r}
leafs_score <- leafs_goals %>%
  rename(interval = created_at) %>%
  arrange(interval) %>%
  mutate(score = 1:nrow(.))

opponent_score <- opponent_goals %>%
  rename(interval = created_at) %>%
  arrange(interval) %>%
  mutate(score = 1:nrow(.))

running_game_score_df <- bind_rows(leafs_score, opponent_score, .id = "team") %>%
  mutate(team = ifelse(team == 1, "leafs", "opponent")) %>%
  spread(team, score) %>%
  fill(leafs, opponent) %>%
  mutate(leafs = replace_na(leafs, 0),
         opponent = replace_na(opponent, 0)) %>%
  bind_rows(data.frame(status_id = "",
                       interval = min(tml_tweet_volume$interval),
                       text = "",
                       leafs = 0,
                       opponent = 0)) %>%
  mutate(running_score = str_glue("{leafs} - {opponent}")) %>%
  mutate(interval = round_date(interval, "2 mins")) %>%
  mutate_if(is.numeric, as.character) %>%
  arrange(interval) 
```

## Base plot

```{r plot, fig.width = 7.2, fig.height = 4, dev = 'CairoPNG', dpi = 300}
base_plot <- full_reddit_data %>%
  ggplot(aes(x = interval, y = thread_volume)) +
    # add logos (in the background)
  annotation_custom(leafs_logo, 
                    xmin = game_start_tweet$created_at + 4250,  # leafs to the left
                    xmax = game_start_tweet$created_at + 100, 
                    ymin=-Inf, ymax=Inf) +
  annotation_custom(opponent_logo,
                    xmin = end_of_game$created_at - 4250, # opponent to the right
                    xmax = end_of_game$created_at - 100,
                    ymin=-Inf, ymax=Inf) +
  
  # add our metadata events as lines like start time and goals
  geom_vline(xintercept = game_start_tweet$created_at) +
  geom_vline(xintercept = end_of_game$created_at) +
  geom_vline(xintercept = leafs_goals$created_at,
             linetype = 5, size = 0.85, colour = leafs_blue) +
  geom_vline(xintercept = opponent_goals$created_at,
             linetype = 2, size = 0.85, colour = opponent_colour) +
  
  # add scoreboard in background
  geom_text(data = running_game_score_df, 
            aes(x = mean(c(game_start_tweet$created_at, end_of_game$created_at)), y = 600, label = running_score), 
            size = 15, alpha = 0.3, colour = leafs_blue, family = "Quantico") +

  # draw the tweet volume lines
  geom_line(data = full_tweet_data, 
            aes(y = thread_volume), color = "#1DA1F2", size = 2) +
  geom_line(color = "#FF7B4D", size = 2) +
  
  # draw the words next
  geom_text(data = full_tweet_data, 
            aes(y = thread_volume, label = word), 
            size = 10, colour = "black", family = "Inter SemiBold", vjust = -0.5) + # twitter
  geom_text(aes(label = word), 
            size = 12, colour = "black", family = "Oswald", vjust = 0.5) + # reddit
  
  ## styling
  #scale_x_datetime(breaks = hourly_ranges$time, 
  #                 labels = time_breaks$time) +
  scale_y_log10() +
  expand_limits(y = 1000, x = c(game_start_tweet$created_at - 1800, end_of_game$created_at + 1800)) +
  
  coord_cartesian(clip = "off") +
  
  labs(title = str_glue("<b style='color:{leafs_blue};font-size:22px'>MAPLE LEAFS</b> <br> <b style='font-size:27px'>CHATTER CHARTS"),
       subtitle = str_glue("<b style='color:#FF4301'>— Reddit Game Thread</b> <b style='color:#1DA1F2'> — Twitter Tweets</b>"),
       x= "", y = "Chat Volume") +
  theme(text = element_text(lineheight = 1),
        panel.grid = element_blank(),
        panel.border = element_rect(fill = NA, colour = grey(0.2), size = 1),
        axis.title.y = element_markdown(size = 8, colour = grey(0.2)),
        axis.ticks = element_line(colour = grey(0.2)),
        plot.title = element_markdown(size = 12, face = "bold",
                                      lineheight = 1, vjust = 1, family = "Montserrat ExtraBold"),
        plot.subtitle = element_markdown(size = 10, family = "Inter ExtraBold", vjust = -20),
        plot.caption = element_text(size = 6, family = "Inter ExtraBold"),
        plot.margin = margin(0.5, 1, 0.1, 1, "cm")) 
```


```{r fig.width = 7.2, fig.height = 4, dev = 'CairoPNG'}
# Add animation
animated_plot <- base_plot +
  transition_reveal(interval) + # follow the interval
  ease_aes("linear") # fixed speed
```

# Save animation

This takes about 20 minutes. It'll beep for you.

```{r saving_animation}
options(gganimate.dev_args = list(height = 4, width = 4*1.8, units = 'in', type = "cairo", res = 144))

animate(plot = animated_plot,
        fps = 20, duration = 15,
        type = "cairo",
        renderer = av_renderer(str_glue("animations/twitter_reddit-leafs-{opponent_file_format}-{game_data$date}22.mp4")))

beepr::beep()
```
